{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Metric Engineering Beyond Accuracy"
      ],
      "metadata": {
        "id": "Mh30Ta1dI8f6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19ea4d60",
        "outputId": "cf7cc46e-47f7-4fa6-bdae-c01e03d3859d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "print(\"Libraries imported and random seed set.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and random seed set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fb2c029",
        "outputId": "a7a96be5-0377-4268-ade9-900821f0f2a1"
      },
      "source": [
        "n_samples = 1000\n",
        "\n",
        "# Generate synthetic features\n",
        "data = {\n",
        "    'user_id': np.arange(n_samples),\n",
        "    'age': np.random.randint(18, 80, n_samples),\n",
        "    'premium': np.random.uniform(500, 3000, n_samples),\n",
        "    'claims_history_score': np.random.uniform(0, 1, n_samples)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Generate imbalanced binary target variable 'is_fraud'\n",
        "base_fraud_probability = 0.05\n",
        "\n",
        "# Adjust probability based on age (e.g., higher for younger/older, but simplified for demonstration)\n",
        "# Let's say slightly higher for younger and slightly higher for older\n",
        "df['fraud_probability'] = base_fraud_probability\n",
        "df.loc[df['age'] < 30, 'fraud_probability'] += 0.02\n",
        "df.loc[df['age'] > 60, 'fraud_probability'] += 0.01\n",
        "\n",
        "# Adjust probability based on claims_history_score (lower score -> higher risk)\n",
        "df['fraud_probability'] -= (df['claims_history_score'] * 0.05) # Subtract more for higher scores\n",
        "df['fraud_probability'] = np.clip(df['fraud_probability'], 0.01, 0.15) # Ensure probabilities are within a reasonable range\n",
        "\n",
        "df['is_fraud'] = (np.random.rand(n_samples) < df['fraud_probability']).astype(int)\n",
        "\n",
        "# Drop the intermediate fraud_probability column\n",
        "df = df.drop(columns=['fraud_probability'])\n",
        "\n",
        "print(f\"Synthetic Insurance Dataset created with {n_samples} samples.\")\n",
        "print(\"Dataset head:\")\n",
        "print(df.head())\n",
        "print(\"\\nFraud distribution:\")\n",
        "print(df['is_fraud'].value_counts(normalize=True))\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic Insurance Dataset created with 1000 samples.\n",
            "Dataset head:\n",
            "   user_id  age      premium  claims_history_score  is_fraud\n",
            "0        0   56  1441.847425              0.380075         0\n",
            "1        1   69  2373.945750              0.197154         0\n",
            "2        2   46  1482.473622              0.479183         0\n",
            "3        3   32  2572.910552              0.194558         0\n",
            "4        4   60  1922.703673              0.166332         0\n",
            "\n",
            "Fraud distribution:\n",
            "is_fraud\n",
            "0    0.966\n",
            "1    0.034\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c2c391",
        "outputId": "3d813773-5110-4401-888d-fefd015aa1a0"
      },
      "source": [
        "def f_beta_score(y_true, y_pred, beta):\n",
        "    \"\"\"\n",
        "    Calculates the F_beta score for binary classification.\n",
        "\n",
        "    Args:\n",
        "        y_true (list or np.array): True labels.\n",
        "        y_pred (list or np.array): Predicted labels.\n",
        "        beta (float): The beta parameter that determines the weight of recall in the combined score.\n",
        "                      beta < 1 lends more weight to precision, while beta > 1 favors recall.\n",
        "                      beta = 1 is the F1 score.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated F_beta score.\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Calculate True Positives (TP), False Positives (FP), False Negatives (FN)\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    # Calculate Precision\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "\n",
        "    # Calculate Recall\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "    # Calculate F_beta score\n",
        "    denominator = (beta**2 * precision) + recall\n",
        "    fbeta = (1 + beta**2) * (precision * recall) / denominator if denominator > 0 else 0\n",
        "\n",
        "    return fbeta\n",
        "\n",
        "print(\"f_beta_score function defined.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f_beta_score function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "467fe674",
        "outputId": "2d92b2d6-aa4d-4647-b330-9a788ab6856e"
      },
      "source": [
        "def mcme_score(y_true, y_pred_proba, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the Mean Confident Misclassification Error (MCME).\n",
        "\n",
        "    Args:\n",
        "        y_true (list or np.array): True labels.\n",
        "        y_pred_proba (list or np.array): Predicted probabilities for the positive class.\n",
        "        y_pred (list or np.array): Predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated MCME score.\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred_proba = np.array(y_pred_proba)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Identify misclassified instances\n",
        "    misclassified_indices = np.where(y_true != y_pred)[0]\n",
        "\n",
        "    if len(misclassified_indices) == 0:\n",
        "        return 0.0 # No misclassifications, MCME is 0\n",
        "\n",
        "    # Calculate confidence of incorrect predictions for misclassified instances\n",
        "    confident_misclassifications = []\n",
        "    for idx in misclassified_indices:\n",
        "        if y_pred[idx] == 1: # Model predicted 1, but true is 0 (FP)\n",
        "            confidence = y_pred_proba[idx] # Confidence in predicting 1\n",
        "        else: # Model predicted 0, but true is 1 (FN)\n",
        "            confidence = 1 - y_pred_proba[idx] # Confidence in predicting 0\n",
        "        confident_misclassifications.append(confidence)\n",
        "\n",
        "    # Calculate the mean of these confidence scores\n",
        "    mcme = np.mean(confident_misclassifications)\n",
        "\n",
        "    return mcme\n",
        "\n",
        "print(\"mcme_score function defined.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mcme_score function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d9361dc",
        "outputId": "071dbf06-30ea-489c-c1b4-3c29f4d57516"
      },
      "source": [
        "def brapl_score(y_true, y_pred, costs_benefits):\n",
        "    \"\"\"\n",
        "    Calculates the Business Risk-Adjusted Profit/Loss (BRAPL).\n",
        "\n",
        "    Args:\n",
        "        y_true (list or np.array): True labels.\n",
        "        y_pred (list or np.array): Predicted labels.\n",
        "        costs_benefits (dict): A dictionary defining the financial impact for each outcome:\n",
        "                               'TP_benefit', 'TN_benefit', 'FP_cost', 'FN_cost'.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated total profit/loss.\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Calculate True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    # Calculate total profit/loss\n",
        "    total_profit_loss = (\n",
        "        (TP * costs_benefits['TP_benefit']) +\n",
        "        (TN * costs_benefits['TN_benefit']) -\n",
        "        (FP * costs_benefits['FP_cost']) -\n",
        "        (FN * costs_benefits['FN_cost'])\n",
        "    )\n",
        "\n",
        "    return total_profit_loss\n",
        "\n",
        "print(\"brapl_score function defined.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "brapl_score function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff91b8d5"
      },
      "source": [
        "## Generate Mock Predictions and Calculate Custom Metrics\n",
        "\n",
        "### Subtask:\n",
        "Generate mock predictions (predicted labels and predicted probabilities) for a binary classification model on the previously created synthetic 'Insurance dataset'. Then, use the implemented `F_beta score`, `Mean Confident Misclassification Error (MCME)`, and `Business Risk-Adjusted Profit/Loss (BRAPL)` functions to calculate their values based on these mock predictions and the true labels.\n",
        "\n",
        "#### Instructions\n",
        "1.  **Generate Mock Predictions:**\n",
        "    *   Create an array `y_true` from the `is_fraud` column of the `df` DataFrame.\n",
        "    *   Generate `y_pred_proba` (predicted probabilities for the positive class) as an array of random floats between 0 and 1, ensuring some correlation with `y_true` to make the predictions somewhat realistic (e.g., higher probabilities for true fraud cases, but with noise).\n",
        "    *   Derive `y_pred` (predicted labels) from `y_pred_proba` using a threshold (e.g., 0.5). Varying the threshold can simulate different model behaviors.\n",
        "2.  **Calculate F_beta Score:**\n",
        "    *   Choose a `beta` value (e.g., 2, to emphasize recall for fraud detection) and call the `f_beta_score` function with `y_true`, `y_pred`, and the chosen `beta`.\n",
        "3.  **Calculate MCME:**\n",
        "    *   Call the `mcme_score` function with `y_true`, `y_pred_proba`, and `y_pred`.\n",
        "4.  **Calculate BRAPL:**\n",
        "    *   Define a `costs_benefits` dictionary with realistic (mock) financial values for `TP_benefit`, `TN_benefit`, `FP_cost`, and `FN_cost` relevant to an insurance fraud scenario.\n",
        "    *   Call the `brapl_score` function with `y_true`, `y_pred`, and the `costs_benefits` dictionary.\n",
        "5.  **Display Results:** Print the calculated values for each of the three custom metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5f564be"
      },
      "source": [
        "**Reasoning**:\n",
        "I will generate mock predictions by taking the 'is_fraud' column as true labels, creating predicted probabilities that are somewhat correlated with the true labels, and then deriving predicted labels using a threshold. This setup will simulate a classification model's output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19f66bb2",
        "outputId": "f803f18b-c919-4597-dc90-5ccbb9bcd72d"
      },
      "source": [
        "y_true = df['is_fraud'].values\n",
        "\n",
        "# Generate mock predicted probabilities (y_pred_proba)\n",
        "# Make them somewhat correlated with y_true for realism\n",
        "np.random.seed(42) # Ensure reproducibility for predictions as well\n",
        "y_pred_proba = np.zeros(len(y_true))\n",
        "for i, true_label in enumerate(y_true):\n",
        "    if true_label == 1: # For true fraud cases, higher probability of predicting fraud\n",
        "        y_pred_proba[i] = np.random.uniform(0.6, 0.95) # High confidence for fraud\n",
        "    else: # For true non-fraud cases, lower probability of predicting fraud\n",
        "        y_pred_proba[i] = np.random.uniform(0.05, 0.4) # Low confidence for fraud\n",
        "\n",
        "# Add some noise/randomness to make it less perfect\n",
        "noise = np.random.normal(0, 0.1, len(y_true))\n",
        "y_pred_proba = np.clip(y_pred_proba + noise, 0, 1)\n",
        "\n",
        "# Derive predicted labels (y_pred) using a threshold\n",
        "threshold = 0.5\n",
        "y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "print(\"Mock predictions generated.\")\n",
        "print(f\"\\nSample y_true (first 10): {y_true[:10]}\")\n",
        "print(f\"Sample y_pred_proba (first 10): {np.round(y_pred_proba[:10], 2)}\")\n",
        "print(f\"Sample y_pred (first 10): {y_pred[:10]}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mock predictions generated.\n",
            "\n",
            "Sample y_true (first 10): [0 0 0 0 0 0 0 0 0 0]\n",
            "Sample y_pred_proba (first 10): [0.2  0.25 0.34 0.32 0.16 0.21 0.15 0.4  0.25 0.13]\n",
            "Sample y_pred (first 10): [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "485daa02"
      },
      "source": [
        "**Reasoning**:\n",
        "I have successfully generated mock predictions. Now, I will calculate the F_beta score using the previously defined `f_beta_score` function, providing a beta value of 2 to emphasize recall for fraud detection, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6539002",
        "outputId": "79bd912d-69e5-4f34-8e66-c38ecdad206c"
      },
      "source": [
        "beta_value = 2\n",
        "f_beta = f_beta_score(y_true, y_pred, beta_value)\n",
        "print(f\"Calculated F-beta (beta={beta_value}) score: {f_beta:.4f}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated F-beta (beta=2) score: 0.8947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b467c437"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step calculated the F-beta score. Now, I will calculate the Mean Confident Misclassification Error (MCME) using the `mcme_score` function, as outlined in the instructions, providing `y_true`, `y_pred_proba`, and `y_pred`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6444bea",
        "outputId": "872c277c-bb3f-4f97-db5b-ddd2b6018841"
      },
      "source": [
        "mcme = mcme_score(y_true, y_pred_proba, y_pred)\n",
        "print(f\"Calculated MCME score: {mcme:.4f}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated MCME score: 0.5430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b67ff15a"
      },
      "source": [
        "**Reasoning**:\n",
        "The F-beta and MCME scores have been calculated. Now, I will calculate the Business Risk-Adjusted Profit/Loss (BRAPL) score. This involves defining a `costs_benefits` dictionary with mock financial values and then calling the `brapl_score` function with `y_true`, `y_pred`, and the `costs_benefits` dictionary, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e9bef40",
        "outputId": "14d75ce7-898d-4548-eda9-d37e515ce3c1"
      },
      "source": [
        "costs_benefits = {\n",
        "    'TP_benefit': 1000,  # Benefit for correctly identifying fraud\n",
        "    'TN_benefit': 50,    # Benefit for correctly identifying non-fraud (e.g., lower processing cost)\n",
        "    'FP_cost': 200,      # Cost for incorrectly flagging non-fraud (e.g., investigation cost, customer dissatisfaction)\n",
        "    'FN_cost': 5000      # Cost for missing actual fraud (e.g., loss from undetected fraud)\n",
        "}\n",
        "\n",
        "brapl = brapl_score(y_true, y_pred, costs_benefits)\n",
        "print(f\"Calculated BRAPL score: ${brapl:.2f}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated BRAPL score: $77300.00\n"
          ]
        }
      ]
    }
  ]
}