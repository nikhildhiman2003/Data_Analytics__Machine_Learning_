{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Manual Model Explainability Engine\n",
        ""
      ],
      "metadata": {
        "id": "oJRcqCJlQN6Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oOu-FBRZHsvN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()"
      ],
      "metadata": {
        "id": "tKxRIEDxOGu5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)"
      ],
      "metadata": {
        "id": "CiLjWz3NORVo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "GDGhtDnGOWgX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "Pdbk1XzbOd0o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test_scaled)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paaOMMtaOhwl",
        "outputId": "b16cbd5c-6f38-460d-965e-34812b4451e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X.columns\n",
        "weights = model.coef_[0]         # shape: (num_features,)\n",
        "intercept = model.intercept_[0]\n",
        "\n",
        "def explain_prediction(x_scaled):\n",
        "    \"\"\"\n",
        "    x_scaled: 1D numpy array of scaled features\n",
        "    returns dataframe of contributions\n",
        "    \"\"\"\n",
        "\n",
        "    # raw contributions\n",
        "    contributions = x_scaled * weights\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"feature\": feature_names,\n",
        "        \"scaled_value\": x_scaled,\n",
        "        \"weight\": weights,\n",
        "        \"raw_contribution\": contributions\n",
        "    })\n",
        "\n",
        "    # total logit\n",
        "    logit = intercept + contributions.sum()\n",
        "\n",
        "    # convert logit to probability\n",
        "    prob = 1 / (1 + np.exp(-logit))\n",
        "\n",
        "    # normalize contributions (absolute percentage of total)\n",
        "    total_abs = np.sum(np.abs(contributions))\n",
        "    df[\"normalized_importance\"] = np.abs(contributions) / total_abs\n",
        "\n",
        "    # sort by impact\n",
        "    df = df.sort_values(\"normalized_importance\", ascending=False)\n",
        "\n",
        "    return df, logit, prob\n"
      ],
      "metadata": {
        "id": "pxcYzIZ9OtyN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 5\n",
        "x = X_test_scaled[sample_idx]\n",
        "\n",
        "explain_df, logit, prob = explain_prediction(x)\n",
        "\n",
        "print(\"\\nPredicted class probability:\", prob)\n",
        "print(\"\\nTop feature contributions:\\n\")\n",
        "print(explain_df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0DkxhGVOxOp",
        "outputId": "5c6ab774-ef72-48e8-a915-511766db46a8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted class probability: 9.480877729723756e-11\n",
            "\n",
            "Top feature contributions:\n",
            "\n",
            "                 feature  scaled_value    weight  raw_contribution  \\\n",
            "21         worst texture      2.291730 -1.350606         -3.095223   \n",
            "7    mean concave points      2.728159 -1.119804         -3.055004   \n",
            "26       worst concavity      3.174988 -0.943053         -2.994182   \n",
            "6         mean concavity      3.306880 -0.801458         -2.650326   \n",
            "28        worst symmetry      1.873723 -1.208200         -2.263833   \n",
            "27  worst concave points      2.311233 -0.778217         -1.798641   \n",
            "5       mean compactness      3.307983  0.540164          1.786853   \n",
            "20          worst radius      1.977724 -0.879840         -1.740081   \n",
            "10          radius error      1.145429 -1.268178         -1.452608   \n",
            "23            worst area      1.664783 -0.841846         -1.401491   \n",
            "\n",
            "    normalized_importance  \n",
            "21               0.093251  \n",
            "7                0.092040  \n",
            "26               0.090207  \n",
            "6                0.079848  \n",
            "28               0.068204  \n",
            "27               0.054189  \n",
            "5                0.053833  \n",
            "20               0.052424  \n",
            "10               0.043763  \n",
            "23               0.042223  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MazkGCS7QF4k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}